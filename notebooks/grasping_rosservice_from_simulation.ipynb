{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a demonstration of the fulle procedure to use a grasping service to execute a grap on a loaded ycb object. \n",
    "The procedure is the following:\n",
    "\n",
    "1.) load an yccb object\n",
    "\n",
    "2.) put the object into the simulated scene and take a picture from a defined viewpoint\n",
    "\n",
    "3.) create a grasp planner request from the recorded data\n",
    "\n",
    "4.) send the request to the ROS grasping service\n",
    "\n",
    "5.) wait for the result and decompose it\n",
    "\n",
    "6.) execute a grasping sequence based on the decomposed result\n",
    "\n",
    "To successfully execute this notebook, a roscore needs to be running on the same machine as the notebook server and the grasping service needs to be running.\n",
    "The grasping services from the `grasping-benchmark-panda` repo can be easily started using the Docker containers provided in the `grasping-benchmark-panda` repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May  2 2023 05:55:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import numpy as np\n",
    "\n",
    "from alr_sim.core import Scene\n",
    "\n",
    "from alr_sim_tools.ycb_utils import YCBLoader\n",
    "from alr_sim_tools.scene_utils import (\n",
    "    execute_grasping_sequence,\n",
    "    record_camera_data,\n",
    "    reset_scene,\n",
    ")\n",
    "from alr_sim_tools.ros_utils import create_grasp_planner_request\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent / \"alr_sim_tools\" / \"ros_msg_srv_definitions\"))\n",
    "from alr_sim_tools.ros_msg_srv_definitions.grasping_benchmarks_ros.srv import (\n",
    "    GraspPlannerResponse,\n",
    "    GraspPlanner,\n",
    ")\n",
    "\n",
    "import rospy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTORY_STRING = \"mj_beta\"\n",
    "OBJECT_POS = (0.5, 0.0, 0.2)\n",
    "CAM_POS = (0.5, 0.0, 1)\n",
    "CAM_QUAT = [0.7071067811865476, 0.0, 0.0, -0.7071067811865475]\n",
    "CAM_HEIGHT = 480\n",
    "CAM_WIDTH = 640\n",
    "INITIAL_ROBOT_POS = (0.0, 0.5, 0.2)\n",
    "INITIAL_ROBOT_QUAT = (0, 1, 0, 0)\n",
    "RENDER_MODE = Scene.RenderMode.HUMAN    \n",
    "DATA_RECORDING_WAIT_TIME = 0.5\n",
    "DATA_RECORDING_MOVE_TIME = 2\n",
    "\n",
    "GRASP_PLANNER_SERVICE_ID = \"contact_bench/contact_grasp_planner_service\"\n",
    "\n",
    "YCB_FOLDER = Path.home() / \"Documents\" / \"SF-ObjectDataset\" / \"YCB\"\n",
    "YCB_OBJECT_ID = \"011_banana\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycb_loader = YCBLoader(ycb_base_folder=YCB_FOLDER, factory_string=\"mj_beta\")\n",
    "\n",
    "sim_obj = ycb_loader.get_ycb_object(\n",
    "    pos=(0.5, 0.0, 0.2), quat=(0, 1, 0, 0), obj_id=YCB_OBJECT_ID, name=YCB_OBJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: mju_openResource: could not open resource '/home/moritz/Documents/SimulationFramework/models/mj/robot/panda_tmp_rb0_9bf73356-ae12-11ee-a7e6-c8ff282b54dd.xml' with default provider at slot 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "camera_data, scene, agent = record_camera_data(\n",
    "    factory_string=FACTORY_STRING,\n",
    "    cam_pos=CAM_POS,\n",
    "    cam_quat=CAM_QUAT,\n",
    "    cam_height=CAM_HEIGHT,\n",
    "    cam_width=CAM_WIDTH,\n",
    "    robot_pos=INITIAL_ROBOT_POS,\n",
    "    robot_quat=INITIAL_ROBOT_QUAT,\n",
    "    object_list=[sim_obj],\n",
    "    target_obj_name=YCB_OBJECT_ID,\n",
    "    render_mode=RENDER_MODE,\n",
    "    wait_time=DATA_RECORDING_WAIT_TIME,\n",
    "    move_duration=DATA_RECORDING_MOVE_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node(\"grasp_planner_client\")\n",
    "importlib.reload(logging)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_req = create_grasp_planner_request(\n",
    "    rgb_img=camera_data[\"rgb_img\"],\n",
    "    depth_img=camera_data[\"depth_img\"],\n",
    "    seg_img=camera_data[\"seg_img\"],\n",
    "    pc_points=camera_data[\"point_cloud_points\"],\n",
    "    pc_colors=camera_data[\"point_cloud_colors\"],\n",
    "    cam_pos=camera_data[\"cam_pos\"],\n",
    "    cam_quat=camera_data[\"cam_quat\"],\n",
    "    cam_intrinsics=camera_data[\"cam_intrinsics\"],\n",
    "    cam_height=CAM_HEIGHT,\n",
    "    cam_width=CAM_WIDTH,\n",
    "    num_of_candidates=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.wait_for_service(GRASP_PLANNER_SERVICE_ID, timeout=30.0)\n",
    "grasp_planner = rospy.ServiceProxy(GRASP_PLANNER_SERVICE_ID, GraspPlanner)\n",
    "\n",
    "reply: GraspPlannerResponse = grasp_planner(grasp_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grasp = sorted(reply.grasp_candidates, key=lambda x: x.score.data)[-1]\n",
    "\n",
    "grasp_pos = best_grasp.pose.pose.position\n",
    "grasp_pos = np.array((grasp_pos.x, grasp_pos.y, grasp_pos.z))\n",
    "\n",
    "grasp_quat = best_grasp.pose.pose.orientation\n",
    "grasp_quat = np.array((grasp_quat.w, grasp_quat.x, grasp_quat.y, grasp_quat.z))\n",
    "\n",
    "grasp_rot = R.from_quat(grasp_quat[[1,2,3,0]]).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Going to home position (0.5, 0, 0.5)\n",
      "INFO:root:Going to hover_ position [ 0.53189097 -0.06252778  0.05163424]\n",
      "INFO:root:Opening gripper\n",
      "INFO:root:Going to grasp position [ 0.53189097 -0.06252778  0.00163424]\n",
      "INFO:root:Closing gripper\n",
      "INFO:root:Going to hover position [ 0.53189097 -0.06252778  0.05163424]\n",
      "INFO:root:Going to home position (0.5, 0, 0.5)\n",
      "INFO:root:Going to drop position (0, 0.5, 0.5)\n",
      "INFO:root:Opening gripper\n",
      "INFO:root:Closing gripper\n"
     ]
    }
   ],
   "source": [
    "execute_grasping_sequence(\n",
    "    agent = agent,\n",
    "    grasp_pos = grasp_pos,\n",
    "    grasp_quat = grasp_quat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_scene(FACTORY_STRING, scene, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "648fd5414dd759cf455503f1ca8a8d1f6103f81afa3f898375d059feb8b10734"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
