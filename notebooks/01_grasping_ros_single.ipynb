{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 28 2022 16:11:27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "import rospy\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import numpy as np\n",
    "\n",
    "from alr_sim.core import Scene\n",
    "\n",
    "from grasp_benchmark.utils.ycb_utils import YCBLoader\n",
    "from alr_simulation_tools.scene_utils import execute_grasping_sequence, create_sample_data, reset_scene\n",
    "\n",
    "from grasping_benchmarks_ros.srv import (\n",
    "    GraspPlanner,\n",
    "    GraspPlannerResponse,\n",
    ")\n",
    "\n",
    "from grasp_benchmark.utils.ros_utils import (\n",
    "    create_grasp_planner_request,\n",
    "    calc_point_cloud_from_images\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scene parameters\n",
    "\n",
    "factory_string = \"mj_beta\"\n",
    "object_pos = (0.5, 0.0, 0.2)\n",
    "adhust_object_position = True\n",
    "cam_pos = (0.5, 0.0, 1)\n",
    "cam_quat = [0.7071067811865476, 0.0, 0.0, -0.7071067811865475]\n",
    "# cam_quat = R.from_euler(\"xyz\", (0, 0, -90), degrees=True).as_quat()[\n",
    "#         [3, 0, 1, 2]\n",
    "#     ].tolist(),\n",
    "# cam_quat = R.from_euler(\"zyx\", (-90, -30, 0), degrees=True).as_quat()[\n",
    "#         [3, 0, 1, 2]\n",
    "#     ].tolist(),\n",
    "# cam_quat = (\n",
    "#     R.from_euler(\"zyx\", (90, 50, 0), degrees=True).as_quat()[[3, 0, 1, 2]].tolist()\n",
    "# )\n",
    "cam_height = 480\n",
    "cam_width = 640\n",
    "robot_pos = (0.0, 0.5, 0.2)\n",
    "robot_quat = (0, 1, 0, 0)\n",
    "render_mode = Scene.RenderMode.HUMAN    \n",
    "wait_time = 0.5\n",
    "move_duration = 2\n",
    "\n",
    "#\n",
    "# grasp_planner_service_id = \"grconvnet_bench/grconvnet_grasp_planner_service\"\n",
    "# grasp_planner_service_id = \"ggcnn_bench/ggcnn_grasp_planner_service\"\n",
    "grasp_planner_service_id = \"contact_bench/contact_grasp_planner_service\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycb_loader = YCBLoader(\n",
    "    ycb_base_folder=Path.home() / \"Documents\" / \"SF-ObjectDataset\" / \"YCB\", factory_string=\"mj_beta\"\n",
    ")\n",
    "# obj_ids = [p.name for p in ycb_loader.ycb_base_folder.iterdir() if p.is_dir()]\n",
    "obj_id = \"011_banana\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node(\"grasp_planner_client\")\n",
    "\n",
    "importlib.reload(logging)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_obj = ycb_loader.get_ycb_object(pos = (0.5, 0.0, 0.2), quat = (0,1,0,0), obj_id=obj_id, name=obj_id)\n",
    "\n",
    "results, scene, agent = create_sample_data(\n",
    "    factory_string=factory_string,\n",
    "    cam_pos=cam_pos,\n",
    "    cam_quat=cam_quat,\n",
    "    cam_height=cam_height,\n",
    "    cam_width=cam_width,\n",
    "    robot_pos=robot_pos,\n",
    "    robot_quat=robot_quat,\n",
    "    object_list=[sim_obj],\n",
    "    target_obj_name=obj_id,\n",
    "    render_mode=render_mode,\n",
    "    wait_time=wait_time,\n",
    "    move_duration=move_duration,\n",
    ")\n",
    "\n",
    "pc_points, pc_colors = calc_point_cloud_from_images(\n",
    "    rgb_img=results[\"rgb_img\"], depth_img=results[\"depth_img\"], cam_intrinsics=results[\"cam_intrinsics\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grasp_req = create_grasp_planner_request(\n",
    "    rgb_img=results[\"rgb_img\"],\n",
    "    depth_img=results[\"depth_img\"],\n",
    "    seg_img=results[\"seg_img\"],\n",
    "    pc_points=pc_points,\n",
    "    pc_colors=pc_colors,\n",
    "    cam_pos=results[\"cam_pos\"],\n",
    "    cam_quat=results[\"cam_quat\"],\n",
    "    cam_intrinsics=results[\"cam_intrinsics\"],\n",
    "    cam_height=cam_height,\n",
    "    cam_width=cam_width,\n",
    "    num_of_candidates=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.wait_for_service(grasp_planner_service_id, timeout=30.0)\n",
    "grasp_planner = rospy.ServiceProxy(grasp_planner_service_id, GraspPlanner)\n",
    "\n",
    "reply: GraspPlannerResponse = grasp_planner(grasp_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grasp = sorted(reply.grasp_candidates, key=lambda x: x.score.data)[-1]\n",
    "\n",
    "grasp_pos = best_grasp.pose.pose.position\n",
    "grasp_pos = (grasp_pos.x, grasp_pos.y, grasp_pos.z)\n",
    "\n",
    "grasp_quat = best_grasp.pose.pose.orientation\n",
    "grasp_quat = np.array((grasp_quat.w, grasp_quat.x, grasp_quat.y, grasp_quat.z))\n",
    "\n",
    "grasp_rot = R.from_quat(grasp_quat[[1, 2, 3, 0]]).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.094238    0.98190363 -0.16426948]\n",
      " [ 0.98183094 -0.11896363 -0.14783657]\n",
      " [-0.16470336 -0.14735304 -0.97527426]]\n"
     ]
    }
   ],
   "source": [
    "hover_pos = grasp_pos + np.array([0, 0, 0.1])\n",
    "agent.gotoCartPositionAndQuat(grasp_pos, grasp_quat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Going to home position (0.5, 0, 0.5)\n",
      "INFO:root:Going to hover_ position [ 0.53091522 -0.06242675  0.04629843]\n",
      "INFO:root:Opening gripper\n",
      "INFO:root:Going to grasp position (0.5309152193367481, -0.06242675334215163, -0.0037015676498408645)\n",
      "INFO:root:Closing gripper\n",
      "INFO:root:Going to hover position [ 0.53091522 -0.06242675  0.04629843]\n",
      "INFO:root:Going to home position (0.5, 0, 0.5)\n",
      "INFO:root:Going to drop position (0, 0.5, 0.5)\n",
      "INFO:root:Opening gripper\n",
      "INFO:root:Closing gripper\n"
     ]
    }
   ],
   "source": [
    "# execute_grasping_sequence(\n",
    "#     agent = agent,\n",
    "#     grasp_pos = grasp_pos,\n",
    "#     grasp_quat = grasp_quat\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_scene(factory_string, scene, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef7e1541b8e7b6b5672ac838d3c045be09c9245709d40ce12336bbbdd1b51144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
